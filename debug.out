=================================================-Fri Mar 29 04:22:21 PM UTC 2024=========================================================
[2024-03-29 12:22:23,298] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:22:24,855] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:22:24,856] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:22:26,796] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:22:29,943] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:22:29,943] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:22:29,943] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:22:29,943] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:22:29,943] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:22:32,935] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:22:33,955] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:22:33,955] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
ModelArguments(model_name_or_path='lmsys/vicuna-13b-v1.5', version='v1', freeze_backbone=False, tune_mm_mlp_adapter=False, vision_towers=['openai/clip-vit-large-patch14-336', 'facebook/dinov2-large'], mm_vision_select_layer=-2, pretrain_mm_mlp_adapter='./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 819, in train
    if model_args.vision_tower is not None:
AttributeError: 'ModelArguments' object has no attribute 'vision_tower'. Did you mean: 'vision_towers'?
[2024-03-29 12:22:34,949] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2755495
[2024-03-29 12:22:34,950] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1
=================================================-Fri Mar 29 04:23:29 PM UTC 2024=========================================================
[2024-03-29 12:23:31,411] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:23:32,888] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:23:32,889] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:23:34,813] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:23:37,993] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:23:37,993] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:23:37,993] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:23:37,993] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:23:37,993] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:23:41,033] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:23:42,074] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:23:42,074] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards:  33%|███▎      | 1/3 [03:18<06:36, 198.29s/it][2024-03-29 12:27:02,109] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:27:03,459] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:27:03,459] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-03-29 12:27:05,383] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:27:06,922] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:27:06,922] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:27:06,922] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:27:06,922] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:27:06,922] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:27:09,795] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:27:10,837] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:27:10,837] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:436] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:436] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:472] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 793, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/akane38/LLaVA/transformers/src/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 137, in __init__
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/home/akane38/LLaVA/transformers/src/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1871, in _setup_devices
    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/state.py", line 170, in __init__
    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 120, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 146, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1141, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 241, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 172, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[2024-03-29 12:27:11,928] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2767012
[2024-03-29 12:27:11,928] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1

=================================================-Fri Mar 29 04:28:14 PM UTC 2024=========================================================

[2024-03-29 12:28:16,554] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:28:18,119] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:28:18,120] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:28:19,970] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:28:21,668] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:28:21,668] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:28:21,668] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:28:21,668] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:28:21,668] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:28:24,642] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:28:25,733] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:28:25,733] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:436] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:436] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:472] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 794, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/akane38/LLaVA/transformers/src/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 137, in __init__
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/home/akane38/LLaVA/transformers/src/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1871, in _setup_devices
    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/state.py", line 170, in __init__
    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 120, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 146, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1141, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 241, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 172, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[2024-03-29 12:28:26,674] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2770161
[2024-03-29 12:28:26,675] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

Downloading shards:  67%|██████▋   | 2/3 [06:38<03:19, 199.34s/it][2024-03-29 12:30:30,407] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2758305
[2024-03-29 12:30:30,407] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -15

=================================================-Fri Mar 29 04:30:42 PM UTC 2024=========================================================

[2024-03-29 12:30:44,410] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:30:45,914] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:30:45,914] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:30:47,838] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:30:51,146] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:30:51,146] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:30:51,146] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:30:51,146] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:30:51,146] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:30:54,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:30:54,992] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:30:54,992] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards:  50%|█████     | 1/2 [00:26<00:26, 26.96s/it]
Downloading shards: 100%|██████████| 2/2 [00:35<00:00, 16.13s/it]
Downloading shards: 100%|██████████| 2/2 [00:35<00:00, 17.76s/it]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 12:31:32,301] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.73s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 912, in train
    if model_args.vision_tower is not None:
AttributeError: 'ModelArguments' object has no attribute 'vision_tower'. Did you mean: 'vision_towers'?
[2024-03-29 12:31:40,195] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2776101
[2024-03-29 12:31:40,196] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

=================================================-Fri Mar 29 04:32:32 PM UTC 2024=========================================================

[2024-03-29 12:32:34,633] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:32:36,097] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:32:36,098] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:32:38,038] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:32:41,262] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:32:41,262] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:32:41,262] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:32:41,262] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:32:41,262] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:32:44,195] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:32:45,249] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:32:45,252] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 12:32:46,816] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.85s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.48s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.69s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 914, in train
    if model_args.vision_tower is not None:
AttributeError: 'ModelArguments' object has no attribute 'vision_tower'. Did you mean: 'vision_towers'?
[2024-03-29 12:32:53,276] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2780731
[2024-03-29 12:32:53,276] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 04:39:22 PM UTC 2024 =========================================================

[2024-03-29 12:39:24,429] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:39:26,026] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:39:26,026] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:39:27,958] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:39:31,063] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:39:31,063] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:39:31,064] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:39:31,064] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:39:31,064] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:39:33,990] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 1, in <module>
    from llava.train.multi_ve_train import train
  File "/home/akane38/LLaVA/llava/__init__.py", line 1, in <module>
    from .model import LlavaLlamaForCausalLM
ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/home/akane38/LLaVA/llava/model/__init__.py)
[2024-03-29 12:39:35,069] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2796221
[2024-03-29 12:39:35,069] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 04:40:50 PM UTC 2024 =========================================================

[2024-03-29 12:40:52,446] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:40:53,981] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:40:53,981] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:40:55,875] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:40:58,955] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:40:58,955] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:40:58,955] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:40:58,956] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:40:58,956] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:41:01,833] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:41:02,836] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:41:02,836] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 12:41:04,318] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.43s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='v1', freeze_backbone=False, tune_mm_mlp_adapter=False, vision_towers=['openai/clip-vit-large-patch14-336', 'facebook/dinov2-large'], mm_vision_select_layer=-2, pretrain_mm_mlp_adapter='./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 51, in initialize_vision_modules
    vision_tower = model_args.vision_tower
AttributeError: 'ModelArguments' object has no attribute 'vision_tower'. Did you mean: 'vision_towers'?
[2024-03-29 12:41:10,969] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2799679
[2024-03-29 12:41:10,969] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 04:47:42 PM UTC 2024 =========================================================

[2024-03-29 12:47:44,476] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:47:45,960] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:47:45,961] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:47:47,902] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:47:51,130] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:47:51,130] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:47:51,130] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:47:51,130] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:47:51,130] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:47:53,992] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:47:55,031] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:47:55,031] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 12:47:58,225] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.66s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='v1', freeze_backbone=False, tune_mm_mlp_adapter=False, multiple_vision_towers=['openai/clip-vit-large-patch14-336', 'facebook/dinov2-large'], mm_vision_select_layer=-2, pretrain_mm_mlp_adapter='./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
['openai/clip-vit-large-patch14-336', 'facebook/dinov2-large']
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 69, in initialize_vision_modules
    self.vision_tower = vision_tower
UnboundLocalError: local variable 'vision_tower' referenced before assignment
[2024-03-29 12:48:05,145] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2815319
[2024-03-29 12:48:05,146] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:07:59 PM UTC 2024 =========================================================

[2024-03-29 13:08:01,882] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:08:03,415] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:08:03,415] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:08:05,278] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:08:08,420] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:08:08,420] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:08:08,420] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:08:08,420] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:08:08,420] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:08:11,331] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:08:12,339] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:08:12,339] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:08:13,870] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.74s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:08:19,937] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 69, in initialize_vision_modules
    self.vision_tower = vision_tower
UnboundLocalError: local variable 'vision_tower' referenced before assignment
[2024-03-29 13:08:21,434] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2859305
[2024-03-29 13:08:21,434] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:09:09 PM UTC 2024 =========================================================

[2024-03-29 13:09:11,743] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:09:13,255] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:09:13,255] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:09:16,065] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:09:19,138] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:09:19,138] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:09:19,138] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:09:19,138] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:09:19,138] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:09:22,052] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:09:23,068] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:09:23,068] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:09:24,803] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.84s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:09:30,897] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:09:35,418] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
[CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
), DINOVisionTower(
  (vision_tower): Dinov2Model(
    (embeddings): Dinov2Embeddings(
      (patch_embeddings): Dinov2PatchEmbeddings(
        (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): Dinov2Encoder(
      (layer): ModuleList(
        (0-23): 24 x Dinov2Layer(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attention): Dinov2Attention(
            (attention): Dinov2SelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): Dinov2SelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (layer_scale1): Dinov2LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): Dinov2MLP(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (activation): GELUActivation()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (layer_scale2): Dinov2LayerScale()
          (drop_path2): Identity()
        )
      )
    )
    (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
)]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 69, in initialize_vision_modules
    self.vision_tower = vision_tower
UnboundLocalError: local variable 'vision_tower' referenced before assignment
[2024-03-29 13:09:37,157] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2862337
[2024-03-29 13:09:37,158] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:23:32 PM UTC 2024 =========================================================

[2024-03-29 13:23:34,288] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:23:35,773] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:23:35,773] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:23:37,690] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:23:40,921] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:23:40,921] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:23:40,922] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:23:40,922] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:23:40,922] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:23:43,873] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:23:44,877] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:23:44,877] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:23:47,319] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.63s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:23:52,976] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:23:53,780] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 103, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:23:55,937] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2894192
[2024-03-29 13:23:55,938] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:24:48 PM UTC 2024 =========================================================

[2024-03-29 13:24:50,741] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:24:52,236] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:24:52,236] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:24:54,168] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:24:57,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:24:57,452] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:24:57,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:24:57,452] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:24:57,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:25:00,590] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:25:01,728] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:25:01,728] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:25:03,300] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:25:09,064] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:25:09,949] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 103, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:25:12,469] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2897495
[2024-03-29 13:25:12,469] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:26:42 PM UTC 2024 =========================================================

[2024-03-29 13:26:44,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:26:45,893] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:26:45,893] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:26:47,795] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:26:50,919] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:26:50,919] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:26:50,919] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:26:50,919] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:26:50,919] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:26:53,831] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:26:54,826] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:26:54,826] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:26:56,368] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.73s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:27:02,225] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:27:03,122] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
ModuleList(
  (0): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (1): DINOVisionTower(
    (vision_tower): Dinov2Model(
      (embeddings): Dinov2Embeddings(
        (patch_embeddings): Dinov2PatchEmbeddings(
          (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): Dinov2Encoder(
        (layer): ModuleList(
          (0-23): 24 x Dinov2Layer(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attention): Dinov2Attention(
              (attention): Dinov2SelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): Dinov2SelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (layer_scale1): Dinov2LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Dinov2MLP(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (activation): GELUActivation()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_scale2): Dinov2LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
  )
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:27:04,935] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2902250
[2024-03-29 13:27:04,935] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:27:48 PM UTC 2024 =========================================================

[2024-03-29 13:27:50,145] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:27:51,695] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:27:51,695] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:27:53,628] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:27:56,657] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:27:56,658] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:27:56,658] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:27:56,658] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:27:56,658] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:27:59,542] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:28:00,551] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:28:00,551] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:28:02,043] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.75s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:28:07,997] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:28:09,082] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
ModuleList(
  (0): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (1): DINOVisionTower(
    (vision_tower): Dinov2Model(
      (embeddings): Dinov2Embeddings(
        (patch_embeddings): Dinov2PatchEmbeddings(
          (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): Dinov2Encoder(
        (layer): ModuleList(
          (0-23): 24 x Dinov2Layer(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attention): Dinov2Attention(
              (attention): Dinov2SelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): Dinov2SelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (layer_scale1): Dinov2LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Dinov2MLP(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (activation): GELUActivation()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_scale2): Dinov2LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
  )
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:28:10,673] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2905220
[2024-03-29 13:28:10,673] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:29:45 PM UTC 2024 =========================================================

[2024-03-29 13:29:46,963] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:29:48,539] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:29:48,539] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:29:50,463] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:29:53,646] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:29:53,646] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:29:53,646] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:29:53,646] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:29:53,646] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:29:56,590] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:29:57,570] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:29:57,570] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:29:59,275] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.36s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.57s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-29 13:30:05,014] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:30:06,033] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
ModuleList(
  (0): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (1): DINOVisionTower(
    (vision_tower): Dinov2Model(
      (embeddings): Dinov2Embeddings(
        (patch_embeddings): Dinov2PatchEmbeddings(
          (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): Dinov2Encoder(
        (layer): ModuleList(
          (0-23): 24 x Dinov2Layer(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attention): Dinov2Attention(
              (attention): Dinov2SelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): Dinov2SelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (layer_scale1): Dinov2LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Dinov2MLP(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (activation): GELUActivation()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_scale2): Dinov2LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
  )
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 914, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:30:07,661] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2910111
[2024-03-29 13:30:07,662] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:31:16 PM UTC 2024 =========================================================

[2024-03-29 13:31:18,613] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:31:20,181] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:31:20,182] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data0/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:31:22,126] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:31:25,424] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:31:25,424] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:31:25,424] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:31:25,424] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:31:25,424] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:31:28,555] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:31:29,691] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:31:29,692] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:31:31,289] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.40s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-29 13:31:36,926] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:31:38,160] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 921, in train
    print(model.get_multiple_vision_towers())
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'MultiVELlavaLlamaForCausalLM' object has no attribute 'get_multiple_vision_towers'
[2024-03-29 13:31:40,440] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2914015
[2024-03-29 13:31:40,440] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data0/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:38:14 PM UTC 2024 =========================================================

[2024-03-29 13:38:16,046] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:38:17,642] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:38:17,643] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data0/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:38:19,558] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:38:22,835] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:38:22,835] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:38:22,835] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:38:22,835] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:38:22,835] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:38:25,715] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:38:26,696] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:38:26,696] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:38:28,340] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-29 13:38:33,929] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:38:34,989] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 968, in train
    data_module = make_supervised_data_module(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 779, in make_supervised_data_module
    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 665, in __init__
    list_data_dict = json.load(open(data_path, "r"))
FileNotFoundError: [Errno 2] No such file or directory: './playground/data/llava_v1_5_mix665k.json'
[2024-03-29 13:38:36,851] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2929555
[2024-03-29 13:38:36,852] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data0/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 03:20:57 AM UTC 2024 =========================================================

scripts/v1_5/multi_ve_finetune.sh: 5: deepspeed: not found

================================================= Sun Mar 31 03:27:45 AM UTC 2024 =========================================================

[2024-03-30 23:27:47,005] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-30 23:27:48,577] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-30 23:27:48,577] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data0/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-30 23:27:50,481] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-30 23:27:53,861] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-30 23:27:53,863] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-30 23:27:53,863] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-30 23:27:53,863] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-30 23:27:53,863] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-30 23:27:56,680] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-30 23:27:57,666] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-30 23:27:57,666] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-30 23:28:00,064] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-30 23:28:05,502] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-30 23:28:06,646] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 968, in train
    data_module = make_supervised_data_module(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 779, in make_supervised_data_module
    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 665, in __init__
    list_data_dict = json.load(open(data_path, "r"))
FileNotFoundError: [Errno 2] No such file or directory: './playground/data/llava_v1_5_mix665k.json'
[2024-03-30 23:28:08,878] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3338325
[2024-03-30 23:28:08,879] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data0/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1
