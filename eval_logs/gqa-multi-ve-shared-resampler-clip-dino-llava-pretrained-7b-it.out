[2024-04-03 00:17:33,529] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
############ llava-v1.5-7b
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.54s/it]
Some weights of the model checkpoint at /data/data0/akane/multi-ve-shared-resampler-clip-dino-llava-pretrained-v1.5-7b/checkpoints/ were not used when initializing MultiVELlavaLlamaForCausalLM: ['model.multiple_vision_towers.0.vision_tower.vision_model.embeddings.class_embedding', 'model.multiple_vision_towers.0.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.post_layernorm.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.post_layernorm.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.pre_layrnorm.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.pre_layrnorm.weight', 'model.multiple_vision_towers.1.vision_tower.embeddings.cls_token', 'model.multiple_vision_towers.1.vision_tower.embeddings.mask_token', 'model.multiple_vision_towers.1.vision_tower.embeddings.patch_embeddings.projection.bias', 'model.multiple_vision_towers.1.vision_tower.embeddings.patch_embeddings.projection.weight', 'model.multiple_vision_towers.1.vision_tower.embeddings.position_embeddings', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.0.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.1.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.10.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.11.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.12.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.13.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.14.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.15.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.16.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.17.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.18.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.19.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.2.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.20.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.21.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.22.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.23.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.3.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.4.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.5.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.6.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.7.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.8.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.attention.attention.key.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.attention.attention.key.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.attention.attention.query.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.attention.attention.query.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.attention.attention.value.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.attention.attention.value.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.attention.output.dense.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.attention.output.dense.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.layer_scale1.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.layer_scale2.lambda1', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.encoder.layer.9.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.layernorm.bias', 'model.multiple_vision_towers.1.vision_tower.layernorm.weight']
- This IS expected if you are initializing MultiVELlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MultiVELlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##### In image init code
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
  0%|          | 0/12578 [00:00<?, ?it/s]/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 1/12578 [00:09<31:58:43,  9.15s/it]  0%|          | 2/12578 [00:21<37:51:16, 10.84s/it]  0%|          | 3/12578 [00:30<35:29:59, 10.16s/it]  0%|          | 4/12578 [00:38<32:59:12,  9.44s/it]  0%|          | 5/12578 [00:47<31:38:07,  9.06s/it]  0%|          | 6/12578 [00:55<30:49:05,  8.82s/it]  0%|          | 7/12578 [01:05<32:08:33,  9.20s/it]  0%|          | 8/12578 [01:16<34:08:58,  9.78s/it]./scripts/v1_5/eval/gqa.sh: line 32: Loop: command not found
Please make sure to use our provided visual features as gqadataset.org for better comparability. We provide both spatial and object-based features trained on GQA train set.
In particular please avoid using features from https://github.com/peteanderson80/bottom-up-attention since they were trained on images contained in the GQA validation set and thus may give false scores improvement.

Please consider using --consistency to compute consistency scores for entailed questions.
If you do so, please provide answers to all questions in val_all_questions.json.

Please consider using --grounding to compute attention scores.
If you do so, please provide attention maps through --attentions.

Loading scene graphs...
Failed to load scene graphs -- cannot evaluate grounding
Loading questions...
Loading choices...
Failed to load choices -- cannot evaluate validity or plausibility
Loading predictions...
no prediction for question 201307251. Please add prediction for all questions.
Traceback (most recent call last):
  File "/data/data1/akane/LLaVA/data/eval/gqa/data/eval.py", line 147, in <module>
    raise Exception("missing predictions")
Exception: missing predictions
